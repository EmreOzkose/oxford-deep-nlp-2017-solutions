{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step0: dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1: load data\n",
    "output:  \n",
    "**input_texts**: list of 2085 talk transcriptions (entire text, not tokenized, mixed case, punctuation etc.)  \n",
    "**labels**: corresponding list of 2085 strings containing several keywords each  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the dataset if it's not already there\n",
    "if not os.path.isfile('ted_en-20160408.zip'):\n",
    "    urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract both the texts and the labels from the xml file\n",
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "texts = doc.xpath('//content/text()')\n",
    "labels = doc.xpath('//head/keywords/text()')\n",
    "del doc\n",
    "#print(input_texts[0])\n",
    "#print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2: preprocessing inputs and labels and building embeddings\n",
    "output:  \n",
    "**inputs_train**: list of 1585 tuples of (token_list, label_integer) for training  \n",
    "**inputs_test**: list of 250 tuples of (token_list, label_integer) for testing  \n",
    "**inputs_cv**: list of 250 tuples of (token_list, label_integer) for cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the texts: lowercase, remove text in parentheses, remove punctuation, tokenize into words (split on whitespace)\n",
    "#removing text in parentheses\n",
    "input_texts = [re.sub(r'\\([^)]*\\)', '', input_text) for input_text in texts]\n",
    "#lowercase\n",
    "input_texts = [input_text.lower() for input_text in input_texts]\n",
    "#remove punctuation\n",
    "input_texts = [re.sub(r'[^a-z0-9]+', ' ', input_text) for input_text in input_texts]\n",
    "#tokenize into words\n",
    "input_texts = [input_text.split() for input_text in input_texts]\n",
    "len(input_texts)\n",
    "#input_texts[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.    671.5  1343.   2014.5  2686.   3357.5  4029.   4700.5  5372.\n",
      "  6043.5  6715. ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Container object of 10 artists>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnKGXpMb9HxoNpBg9cWv9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa893b99358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histogram over input lengths\n",
    "Y_plot, X_plot = np.histogram([len(text) for text in input_texts], bins=10)\n",
    "print(X_plot)\n",
    "X_plot = np.arange(10)\n",
    "plt.bar(X_plot, +Y_plot, facecolor='#9999ff', edgecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now only 2053 inputs left.\n"
     ]
    }
   ],
   "source": [
    "#remove all inputs that have less than 300 tokens in them\n",
    "inputs = zip(input_texts, labels)\n",
    "inputs = [text_and_labels for text_and_labels in inputs if len(text_and_labels[0]) > 300]\n",
    "print(\"There are now only {} inputs left.\".format(len(inputs)))\n",
    "input_texts, labels = zip(*inputs)\n",
    "input_texts, labels = list(input_texts), list(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4470489 tokens in the dataset.\n",
      "There are 18386 tokens that appear only once.\n",
      "There are 18486 unique tokens to remove.\n",
      "It took 0.5890076160430908 seconds to remove all unnecessary items.\n",
      "There are now only 1924078 tokens in the dataset.\n"
     ]
    }
   ],
   "source": [
    "#get list of all words, and feed them into a Counter\n",
    "all_words = [word for input_text in input_texts for word in input_text]\n",
    "print(\"There are {} tokens in the dataset.\".format(len(all_words)))\n",
    "all_words_counter = collections.Counter(all_words)\n",
    "\n",
    "#remove some noise, take away the 100 most common and all words that only appear once\n",
    "most_common_50 = [word for word, count in all_words_counter.most_common(100)]\n",
    "only_once = [word for word, count in all_words_counter.most_common() if count == 1]\n",
    "print(\"There are {} tokens that appear only once.\".format(len(only_once)))\n",
    "\n",
    "to_remove = set(only_once + most_common_50)\n",
    "print(\"There are {} unique tokens to remove.\".format(len(to_remove)))\n",
    "\n",
    "start = time.time()\n",
    "input_texts = [[word for word in input_text if word not in to_remove] for input_text in input_texts]\n",
    "print(\"It took {} seconds to remove all unnecessary items.\".format(time.time()-start))\n",
    "\n",
    "new_all_words = [word for input_text in input_texts for word in input_text]\n",
    "print(\"There are now only {} tokens in the dataset.\".format(len(new_all_words)))\n",
    "\n",
    "#input_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#padding every text to the max text length for later batching\n",
    "l_max = max([len(text) for text in input_texts])\n",
    "for text in input_texts:\n",
    "    text += ['<zero_pad>'] * (l_max - len(text))\n",
    "#print(input_texts[0][-10:-1])\n",
    "#print(np.mean([len(text) for text in input_texts]) == l_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 3, 5, 0, 0, 0, 0, 5, 0, 3, 2, 5, 0, 0, 3, 0, 5, 0]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the labels: search for occurences of the keywords \"technology\", \"entertainment\" or \"design\" and build labels\n",
    "label_lookup = ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']\n",
    "for i in range(len(labels)):\n",
    "    ted_labels = ['o', 'o', 'o']\n",
    "    keyword_list = labels[i].split(', ')\n",
    "    if 'technology' in keyword_list:\n",
    "        ted_labels[0] = 'T'\n",
    "    if 'entertainment' in keyword_list:\n",
    "        ted_labels[1] = 'E'\n",
    "    if 'design' in keyword_list:\n",
    "        ted_labels[2] = 'D'\n",
    "    labels[i] = ''.join(ted_labels)\n",
    "    labels[i] = label_lookup.index(labels[i])\n",
    "len(labels)\n",
    "labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36212"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the unique vocabulary lookup\n",
    "vocab_list = list(set([word for input_text in input_texts for word in input_text]))\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "for i, word in enumerate(vocab_list):\n",
    "    word_to_index[word] = i\n",
    "    index_to_word[i] = word\n",
    "input_indices_list = []\n",
    "for input_text in input_texts:\n",
    "    input_indices_list.append([word_to_index[word] for word in input_text])\n",
    "len(vocab_list)\n",
    "#del vocab_list\n",
    "#del input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load glove word vectors\n",
    "glove = KeyedVectors.load_word2vec_format('glove.6B.50d.w2vformat.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 36212 words\n",
      "found 35096 word vectors, 0.9691814867999559 of our vocabulary\n",
      "missing words e.g. ['cabelguen', 'alisch', 'biochemistries', 'hexapod', 'pranitha', 'moleeds', 'novich', 'cutkosky', 'makey', 'bactericide', 'zacaton', 'kallikuppam', 'rcif', 'lorcainide', 'praxeology', 'itjtawy', 'transgenesis', 'phenome', 'doolikahn', 'atolla', 'homaro', 'lesters', 'gapminder', 'nomological', 'taskrabbit', 'snollygoster', 'haaa', 'nabati', 'drybath', 'trilbies', 'paje', 'elyn', 'zquez', 'drugsheaven', 'radiolaria', 'noraida', 'biorobotics', 'kamkwamba', 'polyphenylalanine', 'volantor', 'repairability', 'myesha', 'nukaks', 'xenophiles', 'maezza', 'complicatedness', 'spinternet', 'sakena', 'smileygirl1978', 'agenticity']\n"
     ]
    }
   ],
   "source": [
    "#creating embeddings, checking for each word in the input texts whether it is part of \n",
    "#the glove corpus, if yes intialize that row in the embeddings with the glove value, if\n",
    "#not initialize it uniformly between [-.1, .1]\n",
    "voc_len = len(word_to_index)\n",
    "print(\"vocabulary size: {} words\".format(voc_len))\n",
    "counter = 0\n",
    "not_found_list = []\n",
    "embeddings = np.random.uniform(-.1, .1, size=(voc_len, 50))\n",
    "for word, index in word_to_index.items():\n",
    "    if word in glove.vocab:\n",
    "        counter += 1\n",
    "        embeddings[index] = glove[word]\n",
    "    elif word == '<zero_pad>':\n",
    "        embeddings[index] = np.zeros(50)\n",
    "    else:\n",
    "        not_found_list.append(word)\n",
    "print(\"found {} word vectors, {} of our vocabulary\".format(counter, float(counter)/voc_len))\n",
    "print(\"missing words e.g. {}\".format(not_found_list[0:50]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1550, 250, 253)\n"
     ]
    }
   ],
   "source": [
    "# combining the tokens and labels for each input, then shuffle them and split into train/test/cv\n",
    "inputs_combined = list(zip(input_indices_list, labels))\n",
    "shuffle(inputs_combined)\n",
    "inputs_train = inputs_combined[:1550]\n",
    "inputs_test = inputs_combined[1550:1800]\n",
    "inputs_cv = inputs_combined[1800:]\n",
    "print((len(inputs_train), len(inputs_test), len(inputs_cv)))\n",
    "#print(inputs_train[0])\n",
    "#print([index_to_word[i] for i in inputs_train[0][0]])\n",
    "#print([input_pair[1] for input_pair in inputs_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa87bb95550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting a histogram over the label distribution in the entire dataset\n",
    "#as you can see 'ooo' is basically ~50% of the dataset, so an accuracy score\n",
    "#of 50% could be reached by simply learning to predict 'ooo' all the time (not good)\n",
    "Y_plot = np.histogram(labels, bins=8)[0]\n",
    "X_plot = np.arange(8)\n",
    "plt.bar(X_plot, +Y_plot, facecolor='#9999ff', edgecolor='white')\n",
    "for x,y in zip(X_plot,Y_plot):\n",
    "    plt.text(x, y+0.05, label_lookup[x], ha='center', va= 'bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3: building the tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building the tensorflow logistic regression model\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextClassifier(object):\n",
    "    def __init__(self, lr, activation, train_algo, embeddings, train_embeddings, voc_len, embed_size, batch_size, hidden_units, classes):\n",
    "        #placeholders\n",
    "        #(batch_size left)\n",
    "        self.input_ph = tf.placeholder(tf.int32, shape=(None, None), name='input')\n",
    "        self.labels_ph = tf.placeholder(tf.int32, shape=(None, classes), name='labels')\n",
    "        self.dropout_ph = tf.placeholder(tf.float32, shape=(), name='dropout')  \n",
    "        \n",
    "        #embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            #depending on whether a pre-trained embedding is provided and whether or not\n",
    "            #the embedding should be trainable\n",
    "            if embeddings is not None and train_embeddings is True:\n",
    "                self.L = tf.Variable(embeddings, name=\"L\")\n",
    "            elif embeddings is not None and train_embeddings is False:\n",
    "                self.L = tf.constant(embeddings, name=\"L\")\n",
    "            else:\n",
    "                self.L = tf.Variable(tf.random_uniform([voc_len, embed_size], -1.0, 1.0), name=\"L\")\n",
    "            input_vectors = tf.nn.embedding_lookup(self.L, self.input_ph)\n",
    "            X = tf.squeeze(tf.reduce_mean(input_vectors, axis=1, keep_dims=True), axis=1)\n",
    "        \n",
    "        #network model\n",
    "        with tf.name_scope(\"network\"):\n",
    "            W1 = tf.Variable(tf.random_normal((embed_size, hidden_units), stddev=0.1), name=\"W1\")\n",
    "            b1 = tf.Variable(tf.zeros(hidden_units), name='b1')\n",
    "\n",
    "            self.W2 = tf.Variable(tf.random_normal((hidden_units, classes), stddev=0.1), name=\"W2\")\n",
    "            b2 = tf.Variable(tf.zeros(classes), name='b2')\n",
    "            \n",
    "            if activation == 'relu':\n",
    "                hidden = tf.nn.relu(tf.matmul(tf.cast(X, tf.float32), W1) + b1)\n",
    "            elif activation == 'tanh':\n",
    "                hidden = tf.nn.tanh(tf.matmul(tf.cast(X, tf.float32), W1) + b1)\n",
    "            else:\n",
    "                hidden = tf.nn.sigmoid(tf.matmul(tf.cast(X, tf.float32), W1) + b1)\n",
    "            hidden = tf.nn.dropout(hidden, self.dropout_ph)\n",
    "\n",
    "            output = tf.matmul(hidden, self.W2) + b2\n",
    "            output = tf.nn.dropout(output, self.dropout_ph)\n",
    "            #yhat = tf.nn.softmax(out) #no need to calc whole prob dist if we only want the argmax\n",
    "            self.predictions = tf.argmax(output, axis=1)\n",
    "        \n",
    "        #loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.losses = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=self.labels_ph)\n",
    "            l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(self.W2) + tf.nn.l2_loss(b2)\n",
    "            self.loss = tf.reduce_mean(self.losses) + (0.01 * l2_loss)\n",
    "            \n",
    "        #acc\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.labels_ph, axis=1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "            \n",
    "        #training operation\n",
    "        with tf.name_scope(\"training\"):\n",
    "            if train_algo == 'adam':\n",
    "                self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "            elif train_algo == 'adagrad':\n",
    "                self.train_op = tf.train.AdagradOptimizer(lr).minimize(self.loss)\n",
    "            else:\n",
    "                self.train_op = tf.train.GradientDescentOptimizer(lr).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, nn, train_data, cv_data, test_data, batch_size, train_dropout, epochs):\n",
    "        self.nn = nn\n",
    "        self.train_data = train_data\n",
    "        self.cv_data = cv_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dropout = train_dropout\n",
    "        self.epochs = epochs\n",
    "        self.W2, self.collect_preds, self.collect_truth = None, [], []\n",
    "\n",
    "    def _get_data_batch(self, curr_index, batch_size, data):\n",
    "        curr_batch = data[curr_index:curr_index+batch_size]\n",
    "        input_batch_list, labels_batch_list = zip(*curr_batch) #unzip the list of input pair tuples (text, label)\n",
    "        curr_input_batch = np.array(input_batch_list, dtype=np.int32)\n",
    "        one_hot = np.zeros((len(labels_batch_list), classes))            \n",
    "        one_hot[range(len(labels_batch_list)), labels_batch_list] = 1            \n",
    "        curr_labels_batch = one_hot\n",
    "        return curr_input_batch, curr_labels_batch\n",
    "    \n",
    "    def _print_status(self, epoch_loss, epoch_train_acc, epoch_cv_acc):\n",
    "        print (\"epoch train loss: {}, epoch train accuracy: {}, epoch cv accuracy: {} \".\n",
    "               format(np.mean(epoch_loss), np.mean(epoch_train_acc), np.mean(epoch_cv_acc)))#, end=\"\\r\")\n",
    "        \n",
    "    def run_epoch(self, sess):\n",
    "        self.W2 = None\n",
    "        epoch_loss, epoch_train_acc, epoch_cv_acc = [], [], []\n",
    "        #run training on the train data\n",
    "        curr_index = 0\n",
    "        while curr_index < len(self.train_data):\n",
    "            curr_input_batch, curr_labels_batch = self._get_data_batch(curr_index, self.batch_size, self.train_data)\n",
    "            feed_dict={self.nn.dropout_ph:self.train_dropout, \n",
    "                       self.nn.input_ph:curr_input_batch, \n",
    "                       self.nn.labels_ph:curr_labels_batch}\n",
    "            self.W2, c_loss, c_losses, c_train_acc, _ = sess.run([self.nn.W2, self.nn.loss, self.nn.losses, self.nn.accuracy, self.nn.train_op], feed_dict=feed_dict)\n",
    "            #print(c_losses)\n",
    "            #print(c_loss)\n",
    "            epoch_loss.append(c_loss)\n",
    "            epoch_train_acc.append(c_train_acc)\n",
    "            curr_index += self.batch_size\n",
    "        \n",
    "        #run cross evaluation on the cv data\n",
    "        curr_index = 0\n",
    "        while curr_index < len(self.cv_data):\n",
    "            curr_input_batch, curr_labels_batch = self._get_data_batch(curr_index, self.batch_size, self.cv_data)\n",
    "            feed_dict={self.nn.dropout_ph:1.0, \n",
    "                       self.nn.input_ph:curr_input_batch, \n",
    "                       self.nn.labels_ph:curr_labels_batch}\n",
    "            c_cv_acc = sess.run(self.nn.accuracy, feed_dict=feed_dict)\n",
    "            epoch_cv_acc.append(c_cv_acc)\n",
    "            curr_index += self.batch_size\n",
    "        \n",
    "        self._print_status(epoch_loss, epoch_train_acc, epoch_cv_acc)\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"Starting training for {} epochs.\".format(self.epochs))\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            for _ in range(self.epochs):\n",
    "                self.run_epoch(sess)\n",
    "            print(\"Done Training.\")\n",
    "            self._test(sess)\n",
    "        \n",
    "    def _test(self, sess):\n",
    "        print(\"Testing the trained model on the test set.\")\n",
    "        #would be better to choose the best model on cv for this instead of simply the one from the last iteration\n",
    "        curr_index = 0\n",
    "        epoch_test_acc = []\n",
    "        while curr_index < len(self.test_data):\n",
    "            curr_input_batch, curr_labels_batch = self._get_data_batch(curr_index, self.batch_size, self.test_data)\n",
    "            feed_dict={self.nn.dropout_ph:1.0, \n",
    "                       self.nn.input_ph:curr_input_batch, \n",
    "                       self.nn.labels_ph:curr_labels_batch}\n",
    "            c_test_acc, test_predictions = sess.run([self.nn.accuracy, self.nn.predictions], feed_dict=feed_dict)\n",
    "            epoch_test_acc.append(c_test_acc)\n",
    "            self.collect_preds.extend(test_predictions)\n",
    "            self.collect_truth.extend(np.argmax(curr_labels_batch, axis=1))\n",
    "            curr_index += self.batch_size\n",
    "        print(\"Test set accuracy: {}\".format(np.mean(epoch_test_acc)))\n",
    "        print(\"Done Testing.\")      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step4: model instantiation, training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 50 epochs.\n",
      "epoch train loss: 2.049983501434326, epoch train accuracy: 0.40089285373687744, epoch cv accuracy: 0.5239999890327454 \n",
      "epoch train loss: 1.6705362796783447, epoch train accuracy: 0.5112500190734863, epoch cv accuracy: 0.5399999618530273 \n",
      "epoch train loss: 1.4753882884979248, epoch train accuracy: 0.5799107551574707, epoch cv accuracy: 0.5800000429153442 \n",
      "epoch train loss: 1.3499927520751953, epoch train accuracy: 0.6247321367263794, epoch cv accuracy: 0.5760000348091125 \n",
      "epoch train loss: 1.3982343673706055, epoch train accuracy: 0.6270535588264465, epoch cv accuracy: 0.6000000238418579 \n",
      "epoch train loss: 1.2956435680389404, epoch train accuracy: 0.6383035778999329, epoch cv accuracy: 0.5680000185966492 \n",
      "epoch train loss: 1.1923316717147827, epoch train accuracy: 0.7072321772575378, epoch cv accuracy: 0.5680000185966492 \n",
      "epoch train loss: 1.0582771301269531, epoch train accuracy: 0.7407142519950867, epoch cv accuracy: 0.5920000076293945 \n",
      "epoch train loss: 1.0310413837432861, epoch train accuracy: 0.7625892162322998, epoch cv accuracy: 0.5679999589920044 \n",
      "epoch train loss: 0.9497240781784058, epoch train accuracy: 0.772232174873352, epoch cv accuracy: 0.5760000348091125 \n",
      "epoch train loss: 0.9122374653816223, epoch train accuracy: 0.7919642925262451, epoch cv accuracy: 0.5239999890327454 \n",
      "epoch train loss: 0.9061110615730286, epoch train accuracy: 0.7887499928474426, epoch cv accuracy: 0.5760000348091125 \n",
      "epoch train loss: 0.8825656175613403, epoch train accuracy: 0.789017915725708, epoch cv accuracy: 0.6000000238418579 \n",
      "epoch train loss: 0.8581994771957397, epoch train accuracy: 0.7986607551574707, epoch cv accuracy: 0.6079999804496765 \n",
      "epoch train loss: 0.7816299796104431, epoch train accuracy: 0.8330357074737549, epoch cv accuracy: 0.5839999914169312 \n",
      "epoch train loss: 0.7838374376296997, epoch train accuracy: 0.823035717010498, epoch cv accuracy: 0.5559999942779541 \n",
      "epoch train loss: 0.827121376991272, epoch train accuracy: 0.8083928823471069, epoch cv accuracy: 0.5759999752044678 \n",
      "epoch train loss: 0.7656433582305908, epoch train accuracy: 0.8182142972946167, epoch cv accuracy: 0.5800000429153442 \n",
      "epoch train loss: 0.792334794998169, epoch train accuracy: 0.8223214149475098, epoch cv accuracy: 0.5879999995231628 \n",
      "epoch train loss: 0.7716180086135864, epoch train accuracy: 0.8205357193946838, epoch cv accuracy: 0.5839999914169312 \n",
      "epoch train loss: 0.8075343370437622, epoch train accuracy: 0.8044643402099609, epoch cv accuracy: 0.5760000348091125 \n",
      "epoch train loss: 0.7684184312820435, epoch train accuracy: 0.8125, epoch cv accuracy: 0.5720000267028809 \n",
      "epoch train loss: 0.8047080039978027, epoch train accuracy: 0.8199999928474426, epoch cv accuracy: 0.5720000267028809 \n",
      "epoch train loss: 0.8093233108520508, epoch train accuracy: 0.822857141494751, epoch cv accuracy: 0.5679999589920044 \n",
      "epoch train loss: 0.8030990958213806, epoch train accuracy: 0.8212499618530273, epoch cv accuracy: 0.5800000429153442 \n",
      "epoch train loss: 0.8955722451210022, epoch train accuracy: 0.7981249690055847, epoch cv accuracy: 0.5399999618530273 \n",
      "epoch train loss: 0.8740482330322266, epoch train accuracy: 0.8172321319580078, epoch cv accuracy: 0.5879999995231628 \n",
      "epoch train loss: 0.8106757998466492, epoch train accuracy: 0.8125892877578735, epoch cv accuracy: 0.5559999942779541 \n",
      "epoch train loss: 0.7491037845611572, epoch train accuracy: 0.8237500190734863, epoch cv accuracy: 0.5400000214576721 \n",
      "epoch train loss: 0.8282956480979919, epoch train accuracy: 0.8091071844100952, epoch cv accuracy: 0.5 \n",
      "epoch train loss: 0.8481073975563049, epoch train accuracy: 0.8153570890426636, epoch cv accuracy: 0.5560000538825989 \n",
      "epoch train loss: 0.8854198455810547, epoch train accuracy: 0.7964285612106323, epoch cv accuracy: 0.5080000162124634 \n",
      "epoch train loss: 0.9403564929962158, epoch train accuracy: 0.7974107265472412, epoch cv accuracy: 0.527999997138977 \n",
      "epoch train loss: 0.9456911683082581, epoch train accuracy: 0.8011607527732849, epoch cv accuracy: 0.4959999918937683 \n",
      "epoch train loss: 0.8794686794281006, epoch train accuracy: 0.8238393068313599, epoch cv accuracy: 0.47600001096725464 \n",
      "epoch train loss: 0.9030059576034546, epoch train accuracy: 0.8018749952316284, epoch cv accuracy: 0.5239999890327454 \n",
      "epoch train loss: 0.8878945112228394, epoch train accuracy: 0.7998213768005371, epoch cv accuracy: 0.43599995970726013 \n",
      "epoch train loss: 0.9995806813240051, epoch train accuracy: 0.7959821224212646, epoch cv accuracy: 0.5720000267028809 \n",
      "epoch train loss: 0.9595118761062622, epoch train accuracy: 0.8010714650154114, epoch cv accuracy: 0.6000000238418579 \n",
      "epoch train loss: 0.8910194635391235, epoch train accuracy: 0.8141964077949524, epoch cv accuracy: 0.5479999780654907 \n",
      "epoch train loss: 0.8825750350952148, epoch train accuracy: 0.8133928775787354, epoch cv accuracy: 0.5119999647140503 \n",
      "epoch train loss: 0.9958746433258057, epoch train accuracy: 0.7927678823471069, epoch cv accuracy: 0.527999997138977 \n",
      "epoch train loss: 0.9863139390945435, epoch train accuracy: 0.8135713934898376, epoch cv accuracy: 0.3959999978542328 \n",
      "epoch train loss: 1.0215309858322144, epoch train accuracy: 0.7880356907844543, epoch cv accuracy: 0.5640000104904175 \n",
      "epoch train loss: 0.9573015570640564, epoch train accuracy: 0.8065178394317627, epoch cv accuracy: 0.6320000290870667 \n",
      "epoch train loss: 0.9366047382354736, epoch train accuracy: 0.8036607503890991, epoch cv accuracy: 0.5920000076293945 \n",
      "epoch train loss: 1.0765843391418457, epoch train accuracy: 0.7830356955528259, epoch cv accuracy: 0.5560000538825989 \n",
      "epoch train loss: 1.0310770273208618, epoch train accuracy: 0.8120535612106323, epoch cv accuracy: 0.5839999914169312 \n",
      "epoch train loss: 1.0620406866073608, epoch train accuracy: 0.8001785278320312, epoch cv accuracy: 0.5440000295639038 \n",
      "epoch train loss: 1.1838542222976685, epoch train accuracy: 0.8012499809265137, epoch cv accuracy: 0.6000000238418579 \n",
      "Done Training.\n",
      "Testing the trained model on the test set.\n",
      "Test set accuracy: 0.6639999747276306\n",
      "Done Testing.\n"
     ]
    }
   ],
   "source": [
    "#config\n",
    "embed_size = 50\n",
    "batch_size = 50\n",
    "hidden_units = 50\n",
    "learning_rate = 0.03\n",
    "voc_len = len(word_to_index)\n",
    "classes = len(label_lookup)\n",
    "\n",
    "\n",
    "#instantiate a network\n",
    "#this can now be tested with all kinds of configurations\n",
    "#'tanh', 'adam', dropout of 0.5 and a lr of 0.05 seems to work best for me\n",
    "nn = TextClassifier(\n",
    "    lr=learning_rate,\n",
    "    activation='tanh',\n",
    "    train_algo='adam',\n",
    "    embeddings=embeddings, #or embeddings=None\n",
    "    train_embeddings=True,\n",
    "    voc_len=voc_len,\n",
    "    embed_size=embed_size,\n",
    "    batch_size=batch_size,\n",
    "    hidden_units=hidden_units,\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "#instantiate a trainer, train the model on the train data and then run the test on the test data\n",
    "trainer = Trainer(\n",
    "    nn=nn,\n",
    "    train_data=inputs_train,\n",
    "    cv_data=inputs_cv,\n",
    "    test_data=inputs_test,\n",
    "    batch_size=batch_size,\n",
    "    train_dropout=0.5,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-e4a9407fff95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'truth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "bins = np.arange(9)\n",
    "plt.hist(np.array(trainer.collect_preds), bins, alpha=0.5, label='predictions')\n",
    "plt.hist(np.array(trainer.collect_truth), bins, alpha=0.5, label='truth')\n",
    "plt.legend(loc='upper right')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([144,  44,  18,  18,   2,  20,   2,   2]),\n",
       " array([ 0.   ,  0.875,  1.75 ,  2.625,  3.5  ,  4.375,  5.25 ,  6.125,  7.   ]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(trainer.collect_truth, bins=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step5: visualizing the hidden to output weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"764e547c-19cb-4230-acde-6d196509aa52\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      document.getElementById(\"764e547c-19cb-4230-acde-6d196509aa52\").textContent = \"BokehJS successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"764e547c-19cb-4230-acde-6d196509aa52\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '764e547c-19cb-4230-acde-6d196509aa52' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"764e547c-19cb-4230-acde-6d196509aa52\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"764e547c-19cb-4230-acde-6d196509aa52\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "plot_W2 = tsne.fit_transform(trainer.W2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <div class=\"bk-plotdiv\" id=\"c0afc1a6-efc1-4dcd-beda-933cf90381e0\"></div>\n",
       "    </div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    var force = false;\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "      window._bokeh_is_loading = undefined;\n",
       "    }\n",
       "  \n",
       "  \n",
       "    \n",
       "    if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "      window._bokeh_timeout = Date.now() + 0;\n",
       "      window._bokeh_failed_load = false;\n",
       "    }\n",
       "  \n",
       "    var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "       \"<div style='background-color: #fdd'>\\n\"+\n",
       "       \"<p>\\n\"+\n",
       "       \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "       \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "       \"</p>\\n\"+\n",
       "       \"<ul>\\n\"+\n",
       "       \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "       \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "       \"</ul>\\n\"+\n",
       "       \"<code>\\n\"+\n",
       "       \"from bokeh.resources import INLINE\\n\"+\n",
       "       \"output_notebook(resources=INLINE)\\n\"+\n",
       "       \"</code>\\n\"+\n",
       "       \"</div>\"}};\n",
       "  \n",
       "    function display_loaded() {\n",
       "      if (window.Bokeh !== undefined) {\n",
       "        document.getElementById(\"c0afc1a6-efc1-4dcd-beda-933cf90381e0\").textContent = \"BokehJS successfully loaded.\";\n",
       "      } else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(display_loaded, 100)\n",
       "      }\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"c0afc1a6-efc1-4dcd-beda-933cf90381e0\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'c0afc1a6-efc1-4dcd-beda-933cf90381e0' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        (function() {\n",
       "          var fn = function() {\n",
       "            var docs_json = {\"7cebf958-e010-4ffe-b28c-0c3c464f090f\":{\"roots\":{\"references\":[{\"attributes\":{\"plot\":{\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"66f3c727-95b0-4585-b6be-9b3bee42bca1\",\"type\":\"BasicTicker\"}},\"id\":\"4b250eb3-e098-4842-ab2f-2a1076a804c0\",\"type\":\"Grid\"},{\"attributes\":{\"plot\":null,\"text\":\"word2vec T-SNE for most common words\"},\"id\":\"23043117-347d-43cd-9b79-efb619b8641e\",\"type\":\"Title\"},{\"attributes\":{\"callback\":null},\"id\":\"2a1d968e-3e91-47d8-9a4e-7fb18f6a48d1\",\"type\":\"DataRange1d\"},{\"attributes\":{\"formatter\":{\"id\":\"90efa3fd-b434-41d0-9f06-d7cc9e2fbb7a\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"66f3c727-95b0-4585-b6be-9b3bee42bca1\",\"type\":\"BasicTicker\"}},\"id\":\"79826c32-1949-43d4-83c4-0ac33229222b\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"}},\"id\":\"032168f6-9125-489b-88ae-f3e2b40916b6\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"a1fc2ed9-37fa-4927-813c-2b4c7eaebbf6\",\"type\":\"ToolEvents\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"eb373cc4-b180-4783-94aa-80489098f13e\",\"type\":\"BasicTicker\"}},\"id\":\"ffccd937-0a0d-4320-8f1d-36485e5fd7cb\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"84a1aa9b-9679-4928-b75a-b0edd3cf2af2\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"943ad68e-3714-4da5-98ff-544608d82925\",\"type\":\"SaveTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"5e4e0fee-778c-4e93-9f6c-c94326d6daa5\",\"type\":\"PanTool\"},{\"id\":\"d7744bda-f98e-4006-87de-22516ace9baa\",\"type\":\"WheelZoomTool\"},{\"id\":\"a8e4ccd6-ddf8-4c45-80e5-bfbd12ef895a\",\"type\":\"ResetTool\"},{\"id\":\"943ad68e-3714-4da5-98ff-544608d82925\",\"type\":\"SaveTool\"}]},\"id\":\"3b7fcd7e-34cf-4da8-a14c-7f2a5bb65a2d\",\"type\":\"Toolbar\"},{\"attributes\":{\"plot\":{\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"source\":{\"id\":\"76aa20d6-caac-4493-8c2c-30d77acb3e8e\",\"type\":\"ColumnDataSource\"},\"text\":{\"field\":\"names\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"#555555\"},\"text_font_size\":{\"value\":\"8pt\"},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"},\"y_offset\":{\"value\":6}},\"id\":\"880d7733-fcd0-41e1-b521-d81cc18933f5\",\"type\":\"LabelSet\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"}},\"id\":\"8b6185b6-0851-49b8-89cd-bde634c2eaf7\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"90efa3fd-b434-41d0-9f06-d7cc9e2fbb7a\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"eb373cc4-b180-4783-94aa-80489098f13e\",\"type\":\"BasicTicker\"},{\"attributes\":{\"below\":[{\"id\":\"79826c32-1949-43d4-83c4-0ac33229222b\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"52b0b810-350c-4e09-ae35-14978a61be5b\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"79826c32-1949-43d4-83c4-0ac33229222b\",\"type\":\"LinearAxis\"},{\"id\":\"4b250eb3-e098-4842-ab2f-2a1076a804c0\",\"type\":\"Grid\"},{\"id\":\"52b0b810-350c-4e09-ae35-14978a61be5b\",\"type\":\"LinearAxis\"},{\"id\":\"ffccd937-0a0d-4320-8f1d-36485e5fd7cb\",\"type\":\"Grid\"},{\"id\":\"b5fbe1cd-c2b3-4f00-a895-8908a51d0bf3\",\"type\":\"GlyphRenderer\"},{\"id\":\"880d7733-fcd0-41e1-b521-d81cc18933f5\",\"type\":\"LabelSet\"}],\"title\":{\"id\":\"23043117-347d-43cd-9b79-efb619b8641e\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"a1fc2ed9-37fa-4927-813c-2b4c7eaebbf6\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"3b7fcd7e-34cf-4da8-a14c-7f2a5bb65a2d\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"370c9b68-8dd5-4bc9-8ac9-07015457d1a9\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"2a1d968e-3e91-47d8-9a4e-7fb18f6a48d1\",\"type\":\"DataRange1d\"}},\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"plot\":{\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"a8e4ccd6-ddf8-4c45-80e5-bfbd12ef895a\",\"type\":\"ResetTool\"},{\"attributes\":{\"formatter\":{\"id\":\"84a1aa9b-9679-4928-b75a-b0edd3cf2af2\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"eb373cc4-b180-4783-94aa-80489098f13e\",\"type\":\"BasicTicker\"}},\"id\":\"52b0b810-350c-4e09-ae35-14978a61be5b\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"5e4e0fee-778c-4e93-9f6c-c94326d6daa5\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"66f3c727-95b0-4585-b6be-9b3bee42bca1\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"76aa20d6-caac-4493-8c2c-30d77acb3e8e\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"032168f6-9125-489b-88ae-f3e2b40916b6\",\"type\":\"Circle\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"8b6185b6-0851-49b8-89cd-bde634c2eaf7\",\"type\":\"Circle\"},\"selection_glyph\":null},\"id\":\"b5fbe1cd-c2b3-4f00-a895-8908a51d0bf3\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null},\"id\":\"370c9b68-8dd5-4bc9-8ac9-07015457d1a9\",\"type\":\"DataRange1d\"},{\"attributes\":{\"plot\":{\"id\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"d7744bda-f98e-4006-87de-22516ace9baa\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"names\",\"x1\",\"x2\"],\"data\":{\"names\":[\"ooo\",\"Too\",\"oEo\",\"ooD\",\"TEo\",\"ToD\",\"oED\",\"TED\"],\"x1\":{\"__ndarray__\":\"0yuxRL8IJz+wDdbOAKkZP3s2K0PIWig/YF4STmzpGD/Y6VkR+snjvjVKUzM+8u4+AHw++O70Ez/Aa126/GQHPw==\",\"dtype\":\"float64\",\"shape\":[8]},\"x2\":{\"__ndarray__\":\"uh3Rh7cKBT8o5djo4RwtP0t1ds+sNRm/QmfxM8d07r7OohgnmXsFP5BSNKTV/SI/GUnVNpE+6j6nCQcGCZsBPw==\",\"dtype\":\"float64\",\"shape\":[8]}}},\"id\":\"76aa20d6-caac-4493-8c2c-30d77acb3e8e\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.4\"}};\n",
       "            var render_items = [{\"docid\":\"7cebf958-e010-4ffe-b28c-0c3c464f090f\",\"elementid\":\"c0afc1a6-efc1-4dcd-beda-933cf90381e0\",\"modelid\":\"9cd8ea97-0cfd-4fce-865e-6c82cf04d57b\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "          };\n",
       "          if (document.readyState != \"loading\") fn();\n",
       "          else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "        })();\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      \n",
       "      if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "        for (var i = 0; i < inline_js.length; i++) {\n",
       "          inline_js[i](window.Bokeh);\n",
       "        }if (force === true) {\n",
       "          display_loaded();\n",
       "        }} else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(run_inline_js, 100);\n",
       "      } else if (!window._bokeh_failed_load) {\n",
       "        console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "        window._bokeh_failed_load = true;\n",
       "      } else if (force !== true) {\n",
       "        var cell = $(document.getElementById(\"c0afc1a6-efc1-4dcd-beda-933cf90381e0\")).parents('.cell').data().cell;\n",
       "        cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "      }\n",
       "  \n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"word2vec T-SNE for most common words\")\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=plot_W2[:,0],\n",
    "                                    x2=plot_W2[:,1],\n",
    "                                    names=label_lookup))\n",
    "\n",
    "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
    "\n",
    "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(labels)\n",
    "\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
